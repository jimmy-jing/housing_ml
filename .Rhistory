knitr::opts_chunk$set(echo = TRUE)
ames <- read.csv('./ames.csv')
class(ames)
summary(ames)
dim(ames)
head(ames)
#Instantiate cleaned/imputed original data set; also drop index column.
ames.orig <- ames[,2:81]
model.saturated <- lm(SalePrice ~ ., data = ames.orig)
summary(model.saturated)
library(MASS)
#prep empty and full models for forward/backward AIC:
model.empty = lm(SalePrice ~ 1, data = ames.orig) #The model with an intercept ONLY.
model.full = lm(SalePrice ~ ., data = ames.orig) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
#run forward AIC: AIC = 29,595.11
forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
dim(ames)
ames.eng <- ames[,2:92]
head(ames.eng)
#create new data set and remove composite features
ames.eng <- ames.eng[, -c(17,18,27,28,30,31,32,33,35,38,43,44,45,46,56,57,61,62,63,64,66,67,68,71,80,82,83,88)]
model.eng.saturated <- lm(TransSalePrice ~ ., data = ames.eng)
summary(model.eng.saturated)
library(glmnet)
library(glmnet)
head(ames.eng)
library(dplyr)
library(dplyr)
unique(ames.eng$MSSubClass)
#change format of MSSubClass from numeric to character type
ames.eng$MSSubClass <- as.character(ames.eng$MSSubClass)
x <- model.matrix(TransSalePrice ~ ., data = ames.eng)
y <- ames.eng$TransSalePrice
set.seed(21)
#Create train and test set:
train = sample(1:nrow(x), 75*nrow(x)/100)
test = (-train)
y.test = y[test]
#test above to ensure accuracy:
length(train)/nrow(x)
length(y.test)/nrow(x)
#create grid for lambda iterations 100,000 - 0.01
grid <- 10^seq(5, -2, length = 100)
#Fit lasso with grid search iterator
lasso.eng.models <- glmnet(x, y, alpha = 1, lambda = grid)
dim(coef(lasso.eng.models))
coef(lasso.eng.models)
#plot feature penalties as lambda decreases
plot(lasso.eng.models, xvar = "lambda", label = TRUE, main = "Lasso Regression")
#CROSS VALIDATION
#fit lasso regressor on training set with grid search on lambda
lasso.models.train <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
lasso.lambda10 <- predict(lasso.models.train, s = 10, newx = x[test, ])
mean((lasso.lambda10 - y.test)^2)
#MSE: 0.1785
lasso.lambda1 <- predict(lasso.models.train, s = 1, newx = x[test, ])
mean((lasso.lambda1 - y.test)^2)
#MSE: 0.1785
lasso.lambda.1 <- predict(lasso.models.train, s = 0.1, newx = x[test, ])
mean((lasso.lambda.1 - y.test)^2)
#MSE: 0.0535 BETTER
lasso.lambda.05 <- predict(lasso.models.train, s = 0.05, newx = x[test, ])
mean((lasso.lambda.05 - y.test)^2)
# k-Folds Cross Validation, k = 10
set.seed(21)
cv.lasso.out <- cv.glmnet(x[test, ], y[test], lambda = grid, alpha = 1, nfolds = 10)
plot(cv.lasso.out, main = "Lasso Regression")
#observation: due to log range of transformed y, we might need to refit a smaller lambda grid
#to validate, let's try a lambda even smaller than our first grid
lasso.lambda.005 <-predict(lasso.models.train, s = 0.005, newx = x[test, ])
mean((lasso.lambda.005 - y.test)^2)
#create grid for lambda iterations 0.01 - (10^-10)
grid2 <- 10^seq(-2, -10, length = 100)
#Fit lasso with grid search iterator
lasso.eng.models <- glmnet(x, y, alpha = 1, lambda = grid2)
dim(coef(lasso.eng.models))
coef(lasso.eng.models)
#plot feature penalties as lambda decreases
plot(lasso.eng.models, xvar = "lambda", label = TRUE, main = "Lasso Regression")
set.seed(21)
cv.lasso.out2 <- cv.glmnet(x[test, ], y[test], lambda = grid2, alpha = 1, nfolds = 10)
cv.lasso.out2 <- cv.glmnet(x[test, ], y[test], lambda = grid2, alpha = 1, nfolds = 10)
plot(cv.lasso.out2, main = "Lasso Regression")
plot(cv.lasso.out2, main = "Lasso Regression")
bestlambda.lasso2 <- cv.lasso.out2$lambda.min
bestlambda.lasso2 #0.00689
log(bestlambda.lasso2)
#What is the test MSE associated with best lasso lambda?
lasso.bestlambdatrain2 <- predict(lasso.models.train, s = bestlambda.lasso2, newx = x[test, ])
mean((lasso.bestlambdatrain2 - y.test)^2)
#Now fit lasso with best lambda and identify coefficients
lasso.model.em2 <- glmnet(x, y, alpha = 1, lambda = bestlambda.lasso2)
predict(lasso.model.em2, type = "coefficients", s = bestlambda.lasso2)
lasso.models.train.cv <- cv.glmnet(x[train, ], y[train], alpha = 1, lambda = grid2)
lasso.models.train.cv <- cv.glmnet(x[train, ], y[train], alpha = 1, lambda = grid2)
max(1 - lasso.models.train.cv$cvm/var(y))
plot(cv.lasso.out2, main = "Lasso Regression")
#plot feature penalties as lambda decreases
plot(lasso.eng.models, xvar = "lambda", label = TRUE, main = "Lasso Regression")
#Now fit lasso with best lambda and identify coefficients
lasso.model.em2 <- glmnet(x, y, alpha = 1, lambda = bestlambda.lasso2)
predict(lasso.model.em2, type = "coefficients", s = bestlambda.lasso2)
plot(cv.lasso.out2, main = "Lasso Regression")
plot(cv.lasso.out2, main = "Lasso Regression")
#plot feature penalties as lambda decreases
plot(lasso.eng.models, xvar = "lambda", label = TRUE, main = "Lasso Regression")
